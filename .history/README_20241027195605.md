# Cell-Cell-Communication

This repository contains the code for the Cell-Cell-Communication project.

```
Cell-Cell-Communication
├── projct
│   ├── earlystopping.py
    └── utils.py
├── main.py
├── preprocessing.py (now not available for project privacy reasons)
└── model.py
```

## Tutorials

This repository contains the following codes and explainations:

- **earlystopping.py**: class for early stopping in training process.
- **utils.py**: models and functions for training and testing.
- **main.py**: code with cross-validation for hyperparameter tuning in regular GAT model.
- **preprocessing.py**: tutorial notebook for preprocessing the data.
- **model.py**: tutorial notebook for training and testing regular GAT model.


### Load packages

Before using those packages in gpu, be sure that all torch-related package is in correct cuda version.
- `nvidia-smi CUDA Version (12.5)`: This version indicates the maximum CUDA version supported by the installed NVIDIA driver. It reflects the driver’s capability to run CUDA applications that are built with any CUDA version up to 12.5.
- `nvcc --version CUDA Version (12.1)`: This version refers to the version of the CUDA toolkit installed on your system. The CUDA toolkit includes the CUDA compiler (nvcc), libraries, and other tools for developing CUDA applications.
- python == 3.10
- torch == 2.1.0+cu121 
- pyg_lib == 0.3.1+pt21cu121
- torch_cluster == 1.6.3+pt21cu121
- torch_scatter == 2.1.2+pt21cu121
- torch_sparse == 0.6.18+pt21cu121
- torch_spline_conv == 1.2.2+pt21cu121


## rGAT Model:

****

- `radius_threshold`: 500~5000
- `interval`: the interval of each radius, setted as 50
- `brain_data`
- `select_family`
- **Other hyperparameters**: `my_hidden_channels`, `my_heads`,`my_dropout`,`my_lr`,`num_epochs`
- The graph attentional operator from the ["Graph Attention Networks"](https://arxiv.org/abs/1710.10903)


$$
        \mathbf{x}^{\prime}_i = \alpha_{i,i}\mathbf{\Theta}_{s}\mathbf{x}_{i} +
        \sum_{j \in \mathcal{N}(i)}
        \alpha_{i,j}\mathbf{\Theta}_{t}\mathbf{x}_{j},
$$


$$
        \alpha_{i,j} =
        \frac{
        \exp\left(\mathrm{LeakyReLU}\left(
        \mathbf{a}^{\top}_{s} \mathbf{\Theta}_{s}\mathbf{x}_i
        + \mathbf{a}^{\top}_{t} \mathbf{\Theta}_{t}\mathbf{x}_j
        \right)\right)}
        {\sum_{k \in \mathcal{N}(i) \cup \{ i \}}
        \exp\left(\mathrm{LeakyReLU}\left(
        \mathbf{a}^{\top}_{s} \mathbf{\Theta}_{s}\mathbf{x}_i
        + \mathbf{a}^{\top}_{t}\mathbf{\Theta}_{t}\mathbf{x}_k
        \right)\right)}.
$$

$$
        \alpha_{i,j} =
        \frac{
        \exp\left(\mathrm{LeakyReLU}\left(
        \mathbf{a}^{\top}_{s} \mathbf{\Theta}_{s}\mathbf{x}_i
        + \mathbf{a}^{\top}_{t} \mathbf{\Theta}_{t}\mathbf{x}_j
        + \mathbf{a}^{\top}_{e} \mathbf{\Theta}_{e} \mathbf{e}_{i,j}
        \right)\right)}
        {\sum_{k \in \mathcal{N}(i) \cup \{ i \}}
        \exp\left(\mathrm{LeakyReLU}\left(
        \mathbf{a}^{\top}_{s} \mathbf{\Theta}_{s}\mathbf{x}_i
        + \mathbf{a}^{\top}_{t} \mathbf{\Theta}_{t}\mathbf{x}_k
        + \mathbf{a}^{\top}_{e} \mathbf{\Theta}_{e} \mathbf{e}_{i,k}
        \right)\right)}.
$$

## Shapes:

- **input:**
  node features:
  $(|\mathcal{V}|, F_{in})$ or
  $((|\mathcal{V_s}|, F_{s}), (|\mathcal{V_t}|, F_{t}))$
  if bipartite,
  edge indices $(2, |\mathcal{E}|)$,
  edge features $(|\mathcal{E}|, D)$ *(optional)*
- **output:** node features $(|\mathcal{V}|, H * F_{out})$ or
  $((|\mathcal{V}_t|, H * F_{out})$ if bipartite.
  If `return_attention_weights=True`, then
  $((|\mathcal{V}|, H * F_{out}),
  ((2, |\mathcal{E}|), (|\mathcal{E}|, H)))$
  or $((|\mathcal{V_t}|, H * F_{out}), ((2, |\mathcal{E}|),
  (|\mathcal{E}|, H)))$ if bipartite



## Before Start... ##

****

- Check if all elements are stored in `cuda`
- Hyperparameter:
    - `my_hidden_channels`: 64
    - `my_heads`: 1
    - `dropout`: 0.6
    - `mu_lr`: learning rate set as 0.01
    - `epoch`: set epoch as 50, deter model from overfitting

### Set Head:

[Reference](https://petar-v.com/GAT/)

To stabilise the learning process of self-attention, we have found multi-head attention to be very beneficial [(as was the case in Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762). Namely, the operations of the layer are independently replicated K times (each replica with different parameters), and outputs are featurewise aggregated (typically by concatenating or adding).
$$
\vec{h^\prime_i} = \Vert^k_{k=1} \sigma \lgroup \sum_{j\in N_i}\alpha^k_{i,j} \mathrm{W}^k \vec{h_j}\rgroup
$$
where $\alpha_{i,j}$ are the attention coefficients derived by the $k$-th replica, and $W_k$ the weight matrix specifying the linear transformation of the $k$-th replica. With the setup of the preceding sections, this fully specifies a **Graph Attention Network (GAT)** layer!

A GAT layer with multi-head attention. Every neighbour $i$ of node 1 sends its own vector of attentional coefficients,$\vec{\alpha_{1i}}$ one per each attention head $\alpha_{1i}^k$. These are used to compute K separate linear combinations of neighbours’ features $\vec{h_i}$, which are then aggregated (typically by concatenation or averaging) to obtain the next-level features of node 1,$\vec{h^\prime_1}$.

### Set Dropout:
Furthermore, we have found that applying dropout [(Srivastava et al., 2014)](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) to the attentional coefficients $\alpha_{i,j}$ was a highly beneficial regulariser, especially for small training datasets. This effectively exposes nodes to stochastically sampled neighbourhoods during training, in a manner reminiscent of the (concurrently published) FastGCN method [(Chen et al., 2018)](https://arxiv.org/abs/1801.10247).

Dropout is a technique that addresses both these issues. It prevents overfitting and provides a way of approximately combining exponentially many different neural network
architectures efficiently. The term “dropout” refers to dropping out units (hidden and visible) in a neural network. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections, as shown in Figure 1.

The choice of which units to drop is random. In the simplest case, each unit is retained with a fixed probability $p$ independent of other units, where $p$ can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks. For the input units, however, the optimal probability of retention is usually closer to 1 than to 0.5

****

## K-Fold

The training data used in the model is split, into k number of smaller sets, to be used to validate the model. The model is then trained on k-1 folds of training set. The remaining fold is then used as a validation set to evaluate the model.

As we will be trying to classify different species of iris flowers we will need to import a classifier model, for this exercise we will be using a DecisionTreeClassifier. We will also need to import CV modules from sklearn.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold, cross_val_score 
clf = DecisionTreeClassifier(random_state=42)
k_folds = KFold(n_splits = 5)
scores = cross_val_score(clf, X, y, cv = k_folds)

```

Assigning X and y:
- X (features): Typically, the x (node features) would be your input X. So in your case, X = data.x, which represents the feature matrix for the nodes.
- y (targets): The y here could refer to either:
- y (downstream genes): These could be your target variables if you are predicting gene expression or similar outcomes.
- labels (embedded cluster names): These could be your target labels if you are performing a clustering or classification task.

Which one to use as y depends on the specific task you are performing:
- If you are predicting gene expression (a multi-output task), use data.y.
- If you are performing node classification (predicting cluster labels), use data.labels.

### Supplementary: How to Get R Square?

**Step1:**
- $y_{\text{true}}$ is the vector of true values.
- $y_{\text{pred}}$ is the vector of predicted values.

**Step2: The R-squared value,  $R^2$ , is calculated as:**
$$
R^2 = 1 - \frac{\sum_{i=1}^{n} \left( y_{\text{true}, i} - y_{\text{pred}, i} \right)^2}{\sum_{i=1}^{n} \left( y_{\text{true}, i} - \bar{y}_{\text{true}} \right)^2}
$$

$$
\bar{y}_{\text{true}} = \frac{1}{n} \sum_{i=1}^{n} y_{\text{true}, i}
$$

**Explanation**

- Residual Sum of Squares (RSS):  $\sum_{i=1}^{n} \left( y_{\text{true}, i} - y_{\text{pred}, i} \right)^2$
- Total Sum of Squares (TSS):  $\sum_{i=1}^{n} \left( y_{\text{true}, i} - \bar{y}_{\text{true}} \right)^2$